---
layout: post
title:  The Nim Raytracer Project &mdash; Part 1
tags: 2016-04-17
---

I've been fascinated with raytracing since my childhood. I still remember the
first time I was playing around with an early 3D modeller/raytracer program on
the Amiga 500 called
[Imagine](https://en.wikipedia.org/wiki/Imagine_%283D_modeling_software%29)
when I was about 13, trying to render some extruded letters made of glass
placed on a classic checkerboard pattern. Well, rendering the final 360x576
([overscan](https://en.wikipedia.org/wiki/Overscan>!) image took a bit more
than 7 hours, but when I turned on the monitor in the morning in great
anticipation and saw the final image in its
[4096-color](https://en.wikipedia.org/wiki/Hold-And-Modify) interlaced glory
(probably quite primitive looking by today's standards), I was in awe! What
made the experience even more interesting for me was that Imagine supported
the use of so-called procedular textures, which are textures generated by
mathematical functions instead of simply using bitmap images. I was
mesmerized---a photorealistic image generated by the computer using nothing
else but pure maths! [^amiga]

## Raytracing vs rasterization

I have always thought of raytracing as some kind of black magic---utterly
fascinating and very scary at the same time because of the complex maths
involved (which turns out, is not quite true). This belief was also
strengthened by my modest excursions into OpenGL programming years later,
which uses a different method called rasterization to generate 3D images.
Simply put, anything more complex than rendering simple 3D shapes (e.g.
reflections, shadows, refractions etc.) require quite a bit of complicated
trickery with rasterization. While rasterization is by several orders of
magnitude more efficient at producing 3D animations at smooth framerates in
realtime, raytracing can produce vastly more photorealistic images. To grossly
oversimply matters, rasterization is very efficient at drawing objects made up
of three-dimensional triangles on a two-dimensional surface (the screen) where
the coloring of those triangles can be controlled in various ways.
Photorealistic rendering that accurately portrays how a given scene would look
in real life is not necessarily of primary importance as long as the end
result looks pleasing (which is a reasonable assumption for lots of fields
such as visualisation and games). It's the 3D artists job to arrange those
colored triangles so that the end result looks good. Most graphics-accelerator
cards today implement some sort of a rasterization pipeline in hardware. 

Raytracing algorithms, however, generate images by simulating the path of
photons emitted by light sources as they bounce among the objects that make up
a three-dimensional scene, finally ending up on the photo-sensitive flat
surface inside the camera. Photorealism is of primary importance, which is
achieved by basing the algorithm of calculating the pixels of the image on the
laws of physics (with certain simplifications, of course, otherwise we would
end up writing an Universe Simulator).


[^amiga]: To put things into perspective, my experimentations with raytracing on the Amiga happened around 1992. There was virtually no Internet back then, I haven't even heard the word until several years later. I read it in a *paper* computer magazine (yes, those things *did* actually exist, can you believe it?) that the CGI effects in the 1991 film "Terminator 2: Judgement Day" were rendered on a room full of ultra high-end (and ultra-expensive) Silicon Graphics IRIS workstations, but even with that kind of computing power it took several *months* to render a few minutes' worth of scenes. Now, seeing *any kind of* semi-photorealistic raytraced image come out of my modest Amiga 500 personal computer sporting a 7.14 Mhz Motorola 68000 CPU seemed nothing short of a small miracle to me then! Oh, and I had no such luxuries as a *hard disk drive* either---the whole program and the scene to be rendered had to fit into the machine's whopping 1 MB of RAM and the final image was then slowly written to a blank 880 KB floppy disk over the course of 7-8 hours!

## Why raytracers

Historically, there have been two main approaches to rendering 3D scenes,
rasterization and raytracing. 



http://blogs.unity3d.com/2014/09/18/global-illumination-in-unity-5/

http://twiik.net/articles/realtime-reflections-in-unity-5

pseudo-refaction
https://vimeo.com/82659909

{% include image.html name="caravaggio.jpg" caption="Surely, <a href=\"https://en.wikipedia.org/wiki/Caravaggio\">Caravaggio</a> did not know about the <a href=\"https://en.wikipedia.org/wiki/Fresnel_equations\">Fresnel equations</a> or the <a href=\"https://en.wikipedia.org/wiki/Metropolis_light_transport\">Metropolis light transport</a> when he painted <a href=\"https://en.wikipedia.org/wiki/The_Calling_of_St_Matthew_(Caravaggio)\">The Calling of Saint Matthew</a> in 1600. Yet no one would say his paintings are not photorealistic enough just because he used \"primitive\" tools such as brushes! (Funnily enough, there's some <a href=\"http://www.webexhibits.org/hockneyoptics/post/grundy7.html\">speculations</a> that he might have used the <a href=\"https://en.wikipedia.org/wiki/Camera_obscura\">camera obscura</a> as well to aid him in attaining his photorealistic results, the workings of which have strong connections to the basic idea of raytracing. Here we go!)" width="80%" %} 


## Coordinate system

We are going to use a [right-handed Cartesian coordinate system
](https://en.wikipedia.org/wiki/Cartesian_coordinate_system#In_three_dimensions)
to represents objects in our 3D world, where the *y-axis* points up, the *x-axis* to the right and the *z-axis* forward. In right-handed coordinate systems, positive rotation is [counterclockwise](https://www.evl.uic.edu/ralph/508S98/coordinates.html) about the axis of rotation.

{% include image.html name="coordinate-system.svg" caption="The left-handed coordinate system used in our renderer. The circular arrow indicates the direction of positive rotation." width="70%" %}

The choice of coordinate system handedness is nothing more than a convention: DirectX,
Unity, Maya and Pixar's RenderMan use left-handed coordinate systems, while OpenGL,
[pbrt](http://www.pbrt.org/) and most other 3D modeling software are
right-handed. For our purposes, compatibiliy with OpenGL and pbrt are the most
important. Also, right-handed coordinate systems are the norm in mathematics,
which will also make our life a bit easier.

## Notation

\$\$\cl\"ma-legend-align\"{\table
s, \text\"scalar\";
\P, \text\"point\";
(\P_x, \P_y, \P_z), \text\"point (by coordinates)\";
\AB, \text\"segment\";
v↖{→}, \text\"vector\";
⟨\P_x, \P_y, \P_z⟩, \text\"vector (by coordinates)\";
v↖{∧}, \text\"unit vector\";
{‖v‖}, \text\"magnitude of vector (length)\";
a·b, \text\"dot product\";
a×b, \text\"cross product\";
\bo M, \text\"matrix\";
}\$\$

Column notation is used for vectors:

\$\$ v↖{→}=[\table x; y; z; 1; ] \$\$

## Transform matrices

Translation and scaling:

\$\$\bo T=[\table
1, 0, 0, t_x;
0, 1, 0, t_y;
0, 0, 1, t_z;
0, 0, 0, 1;
]
\;\;\;\;\;\;\bo S=[\table
s_x,   0,   0, 0;
  0, s_y,   θ, 0;
  0,   θ, s_z, 0;
  0,   0,   0, 1;
]\$\$

Rotations around a given axis:

\$\$\bo R_x=[\table
1,      0,       0, 0;
0, \cos θ, -\sin θ, 0;
0, \sin θ,  \cos θ, 0;
0,      0,       0, 1;
]
\;\;\;\;\;\;\bo R_y=[\table
 \cos θ, 0, \sin θ, 0;
      0, 1,      0, 0;
-\sin θ, 0, \cos θ, 0;
      0, 0,      0, 1;
]\$\$

\$\$\bo R_z=[\table
\cos θ, -\sin θ, 0, 0;
\sin θ,  \cos θ, 0, 0;
     0,       0, 1, 0;
     0,       0, 0, 1;
]\$\$

## Calculating the primary rays

Let $(\P_x, \P_y)$ be the **pixel coordinates** of a pixel of the final image in the framebuffer, $w$ and $h$ the width and the height of the framebuffer in pixels and \$r = w / h\$ the image aspect ratio.

We have to shoot the ray through the middle of the pixels, thus the $(\R_x, \R_y)$ **raster coordinates** of a given pixel are as follows:

\$\$\cl\"ma-join-align\"{\table
\R_x ,= \P_x + 0.5;
\R_y ,= \P_y + 0.5;
}\$\$

From this follows the formula for calculating the $(N_x, N_y)$ **normalized device coordinates (NDC)**:

\$\$\cl\"ma-join-align\"{\table
\N_x ,= \R_x / w r;
\N_y ,= \R_y / h;
}\$\$

And finally the $(S_x, S_y)$ **screen coordinates**:

\$\$\cl\"ma-join-align\"{\table
\S_x ,= 2 \N_x - r;
\S_y ,= -(2 \N_y - 1)
}\$\$

Let $&alpha;$ be the **field of view (FOV)** of the camera. From figure
X it can be seen that by default the field of view is 90&deg; (because $\tan
90&deg; / 2 = \tan 45&deg; = 1 = \BC $), and the length of BC is actually the
$f$ **field of view factor (zoom factor)** of the camera.

\$\$\tan &alpha; / 2 = \BC / \AB = \BC / 1\$\$
\$\$\BC = \tan &alpha; / 2 = f$\$

To obtain the desired field of view, the size of the screen rectangle has to
be scaled by the field of view factor (this is akin to "zooming" with a camera
lens). Thus we yield the **camera coordinates**:

\$\$\cl\"ma-join-align\"{\table
\C_x ,= (2 \N_x - r) f;
\C_y ,= -(2 \N_y - 1) f;
}\$$

After substitutions, the final transform from framebuffer pixel coordinates to camera coordinates looks like this:

\$\$\cl\"ma-join-align\"{\table
\C_x ,= ({2 (\P_x + 0.5) r} / w - r) f;
\C_y ,= (1 - {2 (\P_y + 0.5)} / h) f;
}\$$

So for each pixel $(\P_x, \P_y)$ in the framebuffer we can now calculate the
corresponding camera coordinate $(\C_x, \C_y)$ we'll need to shoot
the ray through. Since the camera (the eye) is at the origin, the direction
vector $d↖{∧}$ of the ray corresponding to pixel $(\P_x, \P_y)$ is simply the
vector $⟨\C_x, \C_y⟩$ normalized:

\$\$ d↖{∧} = ⟨\C_x, \C_y⟩ / {‖⟨\C_x, \C_y⟩‖}\$\$

As the last step, we have to multiply the resulting direction vector with the
camera transform matrix:

\$\$ d↖{∧}' = \bo C d↖{∧}\$\$
