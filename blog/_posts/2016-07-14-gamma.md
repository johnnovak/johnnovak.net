---
layout: post
title:  "What every coder should know about gamma correction"
tags: [graphics, gamma]
date: 2016-07-20
---

## A short quiz

If you have ever written, or are planning to write, *any kind of code* that
deals with digital image processing or the display of digital images on an
electronic device, you should complete the below quiz. If you have answered
one or more questions with a "yes", there's a high chance that your code is
doing the wrong thing and will produce incorrect results. This might not be
immediately obvious to you because these issues can be subtle and they're
easier to spot in some problem domains than in others.

So here's the quiz (hey, don't cheat!):

* I don't know what gamma correction is.
* Gamma is a relic from the CRT display era, now that almost everyone uses
  LCDs, it's safe to ignore it.
* Gamma is only relevant for graphics professionals
  working in the print industry where accurate color reproduction is of great
  importance---for general image processing, it's safe to ignore it.
* The graphics libraries of my operating system handle gamma correctly.[^osgamma]
* The popular graphics library *&lt;insert name here&gt;* I'm using handles gamma correctly.
* Pixels with RGB values of (128, 128, 128) produce half as much light emission as
  pixels with RGB values of (255, 255, 255).
* It is okay to just load pixel data from a popular image format (JPEG, PNG, GIF
  etc.) into a buffer and run image processing algorithms on the raw data
  directly.

[^osgamma]: Only if your operating system is Mac OS X 10.6 or higher.

Well, if you have answered with "yes" to most, don't think you're an idiot!
I would have given a "yes" to most of these questions a week ago myself too.
The truth is, I've been messing around with computer graphics for about
a quarter century now, both as an artist and as a coder, and I had been almost
completely unware of the importance and implications of gamma-correctness
during that whole time.  Somehow, the topic has been just under most computer
users' radar (including programmers writing commercial graphics software), to
the extent that most graphics libraries, image viewers, photo editors and
drawing software of today still don't get gamma right and produce technically
incorrect results.

So nothing to worry about, keep on reading, and by the end of this you'll be
more knowledgeable about gamma than most programmers out there!

## The arcane art of gamma-correctness

Given that vision is arguably the most important sensory input channel for
human-computer interaction, it is quite suprising that gamma correction is one
of the least talked about subjects among programmers and it's mentioned in
technical literature rather infrequently, *including* computer graphics texts.
The fact that most computer graphics textbooks don't explictly mention the
importance of correct gamma handling, or discuss it in practical terms, does
not help matters at all (my [CG textbook from
uni](http://sirkan.iit.bme.hu/~szirmay/szamgraf.html) falls into this
category, I've just checked). Some books mention gamma correction in passing
in somewhat vague and abstract terms, but then neither provide concrete
real-world examples on how to do it properly, nor explain what the
implications of not doing it properly are, nor show how to spot the results of
incorrect gamma handling.

I came across the need for correct gamma handling during writing my [ray
tracer](/tag/ray tracing/) and I had to admit that my understanding of the
topic was rather superficial and incomplete. So I had spent a few days reading
up on it online, but it turned out that many articles about gamma are not much
help either, as many of them are too abstract and confusing, some contain too
many interesting but otherwise irrelevant details, then some lack image
examples or are just simply incorrect or hard to understand. Gamma is not
a terribly difficult concept to begin with, but it's somehow quite hard to
find articles on it that are correct, complete and explain the topic in
a clear language.

## What is gamma and why do we need it?

So here's my attempt to offer a comprehensive explanation of gamma, focusing
just on the most important aspects and assuming no prior knowledge of it.

The image examples in the following sections are assuming that you are viewing
this webpage in a modern browser on a computer monitor that has been
manufactured in the last 30 years or so (CRT or LCD, doesn't matter). Tablets
and phones are generally quite inaccurate compared to proper monitors, so try
to avoid those. You should be viewing the images in a dimly lit room, so no
direct lights or flare on your screen please.

Oh, and also make sure that you're a human. Cats, dogs, primates,
extraterrestrials---please stop reading at this point. (We'll see why this is
not really a joke in the next section!)

### Light emission vs perceptual brightness

Believe it or not, the difference of **light energy emission** between any two
neighbouring vertical bars in the below image is a *constant*. In other words,
the amount of light energy emitted by your screen increases by a *constant
amount* from bar to bar, left to right.

{% include image.html name="linear-ramp32.png" caption="Figure 1 &mdash; Evenly-spaced grayscale bars in terms of emitted light intensity" captionAlign="center" width="100%" %}

Now consider the following image:

{% include image.html name="gamma-ramp32.png" caption="Figure 2 &mdash; Evenly-spaced grayscale bars in terms of perceptual light intensity" captionAlign="center" width="100%" %}

On which image do the gradiation of bars appear more even? It is the
second image! But why is that so? We have just established that in the first
image the bars are evenly (*linearly*) spaced in terms of emitted light
intensity between the darkest black and brightest white your monitor is
capable of reproducing. But why don't we see that as a nice even gradiation
from black to white then? And what is being displayed on the second image
that appears as a linear gradiation to us?

The answer lies in the response of the human eye to light intensity, which is
*nonlinear*. One the first image, the **difference** between the nominal light
intensity of neighbouring bars is constant:

\$\$\Δ_{\linear} = I_n-I_{n-1}\$\$

On the second image, the **ratio** of the nominal light intensity of
neighbouring bars is constant:

\$\$\Δ_{\ratio} = I_n / I_{n-1}\$\$

{: .important}
Because of this, we say that there is a **power law relationship** between
**nominal physical light intensity** and **perceptual brightness**.

All human sensory perception follows a similar [power law
relationship](https://en.wikipedia.org/wiki/Stevens'_power_law) in terms of
the magnitude of stimulus and its perceived intensity.


### Physical vs perceptual linearity

Let's say we wanted to store a representation of the following real-world
object as an image file on the computer (let's pretend for a moment
that perfect grayscale gradients exist in the real world, okay?) Here's how
the "real world object" looks like:

{% include image.html name="linear-ramp.png" caption="Figure 3 &mdash; Ideal smooth grayscale ramp " captionAlign="center" width="100%" %}

Now, let's pretend that we can only store 5-bit grayscale images on this
particular computer system, which gives us 32 distinct shades of gray ranging
from absolute black to absolute white. Also, on this computer, grayscale
values are *proportional* to the light intensity emitted by the display for
a particular value, which will result in a 32-element grayscale as shown on
*Figure 1*. We can say that this grayscale is *linear* in terms of light
emission between successive values.

If we encoded our smooth gradient using only these 32 gray values, we would get
something like this (let's just ignore dither for now to keep things simple):

{% include image.html name="linear-ramp32-perceptual.png" caption="Figure 4 &mdash; Ideal smooth grayscale ramp represented with 32 physically-linear grayscale values" captionAlign="center" width="100%" %}

Well, the transitions are rather abrupt, especially on the left side, because
we only had 32 gray values to work with, but if we squint a little, it's
easy to convince ourselves that this is a more or less "accurate"
representation of the smooth gradient, as far as our limited bit-depth allows
it. But note how the steps are much larger on the left side than on the
right---this is because we are using a grayscale that is *linear* in terms of
*emitted light intensity*, but as we have mentioned before, our eyes don't
perceive light intensity in a linear way!

This observation has some interesting implications. Note how the error between
the original and the 5-bit encoded version is uneven across the image; it's
much larger for dark values than for light ones. We are losing
representational precision for dark values and are using relatively too much
precision for lighter shades.  Clearly, we'd be better off choosing
a different set of 32 grays that would make this error evenly distributed
across the whole range, so both dark and light shades would be represented
with the same precision. If we encoded our original image with such
a grayscale that is *perceptually-linear*, but consequently *non-linear* in
terms of emitted light intensity, and that non-linearity would match that of
our eyes, we'd get the exact same grayscale image we have already seen in
*Figure 2*:

{% include image.html name="gamma-ramp32.png" alt="" caption="Figure 5 &mdash; Ideal smooth grayscale represented with 32 perceptually-linear grayscale values" width="100%" %}

{: .important}
The non-linearity we're talking about here is the **power law** relationship
we mentioned before, and the non-linear transformation we need to apply to our
*physically-linear* grayscale values to transform them into *perceptually-linear*
values is called the **gamma correction**.

### Efficient image encoding

Why is this important? Color data in so-called "true color" bitmap images have
been historically represented with integers, 8-bits per color channel being
the most common (hence a single pixel consisting of the three RGB color
channels gets stored on 24 bits). 8 bits allows us to
represent 256 distinct intensity levels, and if the spacing of these levels were
physically-linear, we would be losing precision on dark values while
being relatively unnecessarily precise on light values, as shown before.

Clearly, this is not ideal. One solution would be to simply keep using the
physically-linear scale and increase the bit depth to 16 bits or more.  This
would double the storage requirements (or worse), which was not an option when
most common image formats were invented, so a different approach was taken.
The idea was to let the 256 distinct levels represent intensity values on
a perceptually-linear scale instead, in which case the vast majority of images
could be adequately represented on just 8 bits per color channel.

{: .important}
The transformation used to represent the physically-linear intensity data
either generated synthetically via an algorithm or captured by a linear
device (such as a CMOS of a digital camera or a scanner) with the discrete
values of the perceptually-linear scale is called **gamma encoding**.

The 24-bit [RGB colour model](https://en.wikipedia.org/wiki/RGB_color_model#Video_framebuffer)
(RGB24) used on virtually all consumer level electronic devices uses 8-bit
[gamma encoded values](https://en.wikipedia.org/wiki/RGB_color_model#Nonlinearity) per
channel to represent light intensities. If you recall what we discussed
earlier, this means that pixels with RGB(128, 128, 128) will *not* emit 50%
the light energy of pixels with RGB(255, 255, 255), but only about 22%! Of
course, if we're talking about *perceptual* light intensity, then RGB(128,
128, 128) *appears* to be half as bright as RGB(255, 255, 255) to the human
eye! If you find this confusing, reflect a bit on it or read this whole
section again, because it's crucial to have a clear understanding what we've
learned so far before progression any further (trust me, it will only get messier).

Of course, gamma encoding is always done with the assumption that the image is
ultimately meant to be viewed by humans on computer screens. For other
purposes, using floats and sticking with the linear scale is often a much
better choice, as we'll later see.

### The gamma transfer function

The process of converting values from linear space to gamma space is called
**gamma encoding** (or *gamma compression*), and the reverse **gamma
decoding** (or *gamma expansion*).

The formulas are very simple, we only need to use the aforementioned power law
function:

\$\$\V_{\encoded} = \V_{\linear} ^ \{1/\γ}\$\$

\$\$\V_{\linear} = \V_{\encoded} ^ \{\γ}\$\$

The **standard gamma (γ)** value to use in computer display systems is
**2.2**. The reason for this is because a gamma of 2.2 approximately matches
the power law sensitivy of human vision. The exact constant varies from person
to person and also depends on the lighting conditions and other factors, but
a standard value had to be chosen and 2.2 was good enough. Don't be too hung
up on this.

Now, a very important point that many texts fail to mention is that the input
values have to be in the 0 to 1 range and the output will be consequently
mapped to the same range too. From this follows the slightly counterproductive
fact that **γ&lt;0** is used for **encoding** (compression) and **γ&gt;0** for
**decoding** (expansion). The below charts demonstrate the gamma transfer
functions for encoding and decoding, plus the linear gamma (γ=1.0) case:

{% include image.html name="gamma.svg" caption="Figure 6 &mdash; Gamma transfer functions<br /><br /> a) encoding gamma, or gamma compression (γ=1/2.2≈0.4545)<br />b) linear gamma (γ=1.0)<br />c) decoding gamma, or gamma expansion (γ=2.2)" captionAlign="center" width="100%" %}

We have only seen grayscale examples so far, but there's nothing special about
RGB images; we just simply need to encode or decode each colour channel with
the same gamma (1/γ for encoding and γ for decoding).

### Gamma vs sRGB

[sRGB](https://en.wikipedia.org/wiki/SRGB) is a colour space that is the
de-facto standard for consumer electronic devices nowadays, including
monitors, digital cameras, scanners, printers and handheld devices. It is
also the standard colour space for images on the Internet.

The sRGB specification defines what gamma to use for encoding and decoding
sRGB images (among other things such as colour gamut, but these are not
relevant to our current discussion). sRGB gamma is very close to a standard
gamma of 2.2, but it has a short linear segment in the very dark range to
avoid a slope of infinity at zero (this is more convenient in numeric
calculations).  The formulas used to convert from linear to sRGB and back can
be found
[here](https://en.wikipedia.org/wiki/SRGB#Specification_of_the_transformationhere).

You don't actually need to understand all these finer details; the important
thing to know is that in 99% of the cases you'll want to use sRGB instead of
plain gamma. The reason for that is that all graphics cards have hardware sRGB
support since 2005 or so, so decoding and encoding is virtually for free. The
native colour space of your monitor is most likely sRGB (unless it's
a professional monitor for graphics, photo or video work) so if you just chuck
an sRGB encoded pixel data into the framebuffer, the resulting image will look
correct on the screen (given the monitor is properly calibrated). Popular image
formats such as JPEG and PNG can store colour space information, but very
often images don't contain such data, in which case virtually all image
viewers and browsers will interpret them as sRGB.

### Processing gamma-encoded images

So if virtually everything defaults to sRGB nowadays, what is exactly the
problem? Looks like we have just wasted our time understanding this whole
gamma business, because if everything uses sRGB, things will just work out
fine at the end without any extra effort on our part, right?

This is surely a very tempting argument, at least superficially, because if
our camera writes sRGB JPEG files, we can just decode the JPEG data, copy it
into the framebuffer of the graphics card and the image would be displayed
correctly on our sRGB LCD monitor (where "correctly" means it would accurately
represent the photographed real-world scene). And you know what, this is
actually 100% true so far!

The problem will happen in the moment when we started running *image
processing* algorithms on our sRGB data directly. Remember, gamma encoding is
a non-linear transformation and sRGB encoding is basically just a funky way of
doing gamma encoding of around γ=2.2 or so. All image processing algorithms
you can find in computer graphics texts will assume *linear data*, unless
explicitly stated, so this means that all your processing will be subtly---or
in some cases not soo subtly---wrong! This includes resizing, blurring,
compositing, interpolating between pixel values, antialiasing and so on---the
list is endless!

Unfortunately, k


### Hey, you forgot about CRT displays?

We haven't discussed CRT display technology so far because it's not strictly
necessary to understand how gamma is used in computer systems of today.

### Conclusion

And this is pretty much all there is to gamma encoding and decoding. To recap,
the only reason to use gamma encoding in digital image storage is to be able
to store images more efficiently that are ultimately meant for human viewing.
It is important to remember that once a linear image data is gamma encoded to
fixed-length integers (typically 8-bits per channel), you cannot recreate the
original linear data from the gamma encoded image because gamma encoding is
a non-linear process. There will always be loss of precision, in particular in
the lighter tones, especially the highlights. In some sense, gamma encoding
can be regarded as the "MP3 of images"; it is an acceptable presentation
final format, but not a good archival or intermediate format for further
processing.

## Effects of gamma-incorrectness

### Gradients

CSS, Photoshop wrong
SVG `color-interpolation` `sRGB` `linearRGB`

https://www.w3.org/TR/SVG/painting.html#ColorInterpolationProperties

### Linear interpolation

### Scaling

### Antialiasing

### Physically-based rendering

## Gamma-awareness is unevenly distributed

Interestingly, my research has revealed that "gamma-awareness" is not
distributed evenly among different developer and user groups. I was able to
identify a handful of mostly non-overlapping distinct groups, primarily based
on the industry they work in:

* Professional compositing, VFX and rendering software used in **high-budget film
  production** has been getting gamma perfectly right for decades now. That's probably
  because it's next to impossible to match CGI content to actual footage with
  a gamma-incorrect workflow and big studios had the budget to hire the best
  people in the industry who more likely had a thorough understanding of computer
  graphics.

* In contrast, **mainstream 3D applications** (Maya, 3ds Max, Cinema 4D etc.)
  although have had various levels of support for gamma for quite a while,
  they have just started to default to gamma-correct workflows from about 2010
  onwards (they call it *Linear Workflow* or *LWF* for short, by the
  way).

* With the popularity of physically based rendering techniques being on the
  rise in recent years, **game developers** seem to have become increasingly
  aware of the importance of handling gamma correctly, as failing to do so
  would make realistic looking renders almost impossible to achieve, similarly
  to films.  Basically, as AAA games are converging to feature films in visual
  quality, gamedev people are rediscovering what the film folks have already
  known for decades.

* **Print industry people** (and DTP-refugees) tend to have a good understanding
  of gamma (well, they have to), although many professional editing software
  (including Photoshop) default to a mostly gamma-incorrect workflow for
  legacy reasons.

* Everyday developers and users of most **simple 2D applications** (image viewers, photo
  editors, drawing programs etc.) are generally blissfully unaware of the
  perils of gamma-incorrectness, as evidenced by the large number of image
  viewers, converters and libraries that produce incorrect output.


Author of the "fltk" toolkit and "flwm" window manager and the Nuke
Compositing System by The Foundry. Was at Digital Domain for almost 15 years,
currently senior software developer at Rhythm & Hues in Los Angeles.

https://tech.slashdot.org/story/10/02/23/2317259/scaling-algorithm-bug-in-gimp-photoshop-others

post from 2010

> My software has been calculating in linear space for over a decade now (this
> is the Nuke Compositor currently produced by The Foundry but at the time it
> was used by Digital Domain for Titanic). You can see some pages I wrote on the
> effect [here](https://spitzak.github.io/conversion/composite.html). See
> [here](https://spitzak.github.io/conversion/index.html) for the overall paper
> and a Siggraph paper on the conversion of such images
> [here](https://spitzak.github.io/conversion/sketches_0265.pdf). In fact a lot
> more work went into figuring out how to get such linear images to show on the
> screen on hardware of that era than on the obvious need to do the math in
> linear. Initial work on this was done for Apollo 13 as the problems with gamma
> were quite obvious when scaling images of small bright objects against the
> black of space.
>
> For typical photographs the effect is not very visible in scaling, as the
> gamma curve is very close to a straight line for two close points and thus the
> result is not very much different. Only widely separated points (ie very high
> contrast images with sharp edges) will show a visible difference. This
> probably means you are trying to scale line art, there are screenshots in the
> html pages showing the results of this. Far worse errors can be found in
> lighting calculations and in filtering operations such as blur. At the time
> even the most expensive professional 3D renderers were doing lighting
> completely wrong, but things have gotten better now that they can use floating
> point intermediate images.
>
> One big annoyance is that you better do the math in floating point. Even 16
> bits is insufficient for linear light levels as the black points will be too
> far apart and visible (the space is wasted on many many more white levels than
> you ever would need). A logarithmic system is needed, and on modern hardware
> you might as well use IEEE floating point, or the ILM "half" standard for
> 16-bit floating point.

https://www.thefoundry.co.uk/products/nuke/

- - -

{::options parse_block_html="true" /}
<section class="links">

## References & further reading

### General gamma/sRGB info

{: .compact}
* [Charles Poynton -- Gamma FAQ](http://www.poynton.com/GammaFAQ.html)
* [Cambridge in Colour -- Understanding gamma correction](http://www.cambridgeincolour.com/tutorials/gamma-correction.htm)
* [Tom Forsynth -- The sRGB Learning Curve](https://gamedevdaily.io/the-srgb-learning-curve-773b7f68cf7a#.ssgyxju0h)
* [Eric Brasseur -- Gamma error in picture scaling](http://www.4p8.com/eric.brasseur/gamma.html)
* [Wikipedia -- Gamma correction](https://en.wikipedia.org/wiki/Gamma_correction)
* [Wikipedia -- sRGB](https://en.wikipedia.org/wiki/SRGB)
* [Wikipedig -- RGB color model](https://en.wikipedia.org/wiki/RGB_color_model)

### Linear lighting & workflow (LWF)

{: .compact}
* [In the Games of Madness -- Tech Feature: Linear-space lighting](https://frictionalgames.blogspot.com/2013/11/tech-feature-linear-space-lighting.html)
* [Larry Gritz, GPU Gems 3 -- Chapter 24. The Importance of Being Linear](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html)
* [Jeremy Birn -- Top Ten Tips for More Convincing Lighting and Rendering](http://www.peachpit.com/articles/article.aspx?p=2165641) – (Section 1. Use a Linear Workflow)
* [Unity Documentation -- Linear Rendering](https://docs.unity3d.com/Manual/LinearLighting.html)
* [Bill Spitzak -- High-speed Conversion of Floating Point Images to 8-bit](https://spitzak.github.io/conversion/index.html)
* [Renderman -- Linear Workflow](https://renderman.pixar.com/view/LinearWorkflow)
* [Nick Campbell -- What Is Linear Workflow and How Can It Help Your Renders Look Better?](http://greyscalegorilla.com/tutorials/what-is-linear-workflow-and-how-can-it-help-your-renders-look-better/)

### Bonus stuff

{: .compact}
* [Lav R. Varshney, John Z. Sun -- Why do we perceive logarithmically?](http://www.rle.mit.edu/stir/documents/VarshneyS_Significance2013.pdf)
* [Wikipedia -- Stevens' power law](https://en.wikipedia.org/wiki/Stevens'_power_law)
* [The Adobe Photoshop Lightroom Book -- The Lightroom RGB space](http://ptgmedia.pearsoncmg.com/imprint_downloads/peachpit/peachpit/lightroom4/pdf_files/LightroomRGB_Space.pdf)
* [OpenEXR](http://www.openexr.com/)
</section>
